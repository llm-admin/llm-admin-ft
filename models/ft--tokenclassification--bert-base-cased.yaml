model_config:
  warmup: True
  model_task: fill-mask
  model_id: bert-base-cased
  max_input_words: 800
  initialization:
    runtime_env:
      pip:
        - deepspeed==0.9.2
        - accelerate
    # s3_mirror_config:
    #   bucket_uri: s3://large-dl-models-mirror/models--amazon--LightGPT/main-safetensors/
    initializer:
      type: Finetune
      dtype: float32
      from_pretrained_kwargs:
        # use_cache: true
        trust_remote_code: true
        num_labels: 9
      # use_kernel: true   # for deepspped type only
      # max_tokens: 1536   # for deepspped type only
ft_config:
  ft_task: "tokenclassification"
  data_config:
    data_path: conll2003
    # train_file:
    # validation_file:
    input_columns: 
      - "sentence"
    validation_column: validation
    # labels
  train_config:
    per_device_train_batch_size: 8
    learning_rate: 2e-5
    num_train_epochs: 2
    weight_decay: 0.01

scaling_config:
  num_workers: 6
  num_gpus_per_worker: 0
  num_cpus_per_worker: 1   # for infrence
  # resources_per_worker:
  #   accelerator_type_cpu: 0.01
  ray_actor_options:
    num_cpus: 0.1
